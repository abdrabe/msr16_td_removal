% -*- root: main.tex -*-

\begin{figure*}[thb!]
  \centering
  \includegraphics[width=0.90\textwidth]{figures/automatically_classified_data_approach2.pdf}
  \caption{Automatically Classified Data Approach Overview}
  \label{fig:automatically_classified_data_approach_overview}
\end{figure*}

The main goal of our study is to understand the removal of \SATD. To perform our study, we select five large, well-commented, open source projects. We then checkout all file versions to enable use to determine when the \SATD was introduced and removed. Then, we use the technique recently presented by Maldonado \emph{et al.}~\cite{Maldonado2015TSE} to detect \SATD that exists in the system. Finally, we use the generated data to perform our case study. Figure~\ref{fig:automatically_classified_data_approach_overview} shows an overview of our approach, and the following subsections detail each step.

%comments after they get introduced into projects, and most specifically, who are the authors responsible for the introduction and removal of these comments. To do that, we divided our study in two main parts. First, we developed a tool that allows us to extract and classify \SATD comments based on natural language processing (NLP) techniques. Using this tool we extracted and classified the comments of five open source projects. Second, we analyze the performance of our tool in terms of precision and recall using two other open source projects which we have manually classified the \SATD comments. 

%In a nutshell the tool first clones the analyzed repository locally. Second, it identifies all Java source code files in the repository. By analyzing the history of the project we are able to identify even files that were already removed. Third, we use a third part library to parse the source code and extract the information that we use in our analysis. Fourth, we apply four filtering heuristics that aims to eliminate non \SATD comments. Lastly, it uses NLP techniques to automatically classify \SATD comments. 



\subsection{Project Data}
\label{sub:project_data_extraction}
The first step to perform our empirical is to select a number of case study projects. Since our approach heavily depends on source code comments, we selected five open source projects, namely Camel, Gerrit, Hadoop, Log4j and Tomcat. We select these projects since they belong to different application domains, are well commented, vary in size, number of contributors, and with a high activity level. Additionally, we selected projects that have git as their source code repository.
% alexander{Shouldn't we relate the choice of the projects to the guidelines of Runeson and H\"{o}rst~\cite{RunesonH09}?} \emad{can we add that here?}


% \alexander{Did we somehow clean up the contributors? The same person might use different emails to commit the changes in git?} 

% \alexander{In all versions in total? or is this a median?}

% \alexander{This heuristics has not been described yet. Why does one need to filter?}

Table ~\ref{tab:project_details} provides details about each of the projects used in our study. The columns of Table~\ref{tab:project_details} presents the number of Java files analyzed, followed by the total source lines of code (SLOC), the number of different file versions that we find analyzing the history of the project, the number of contributors, the number of extracted comments (\textit{i.e.,} from all versions), the number of comments analyzed after applying our filtering heuristics (\textit{i.e.,} removing commented source code, license comments and Javadoc comments), the number of comments that were classified as \SATD and finally the number of unique \SATD comments. To calculate the unique \SATD number we take in consideration only the first time that the comment appears on any of the different file versions. This is necessary because the same comment may appear in different versions of the file. In total, we obtained 7,749,969 comments, found in 446,775 different versions of 30,915 Java classes. The size of the selected projects varies between 30,287 and 800,488 SLOC, and the number of contributors of these projects ranges from 32 to 289. Since there are exist different definitions for the SLOC metric we clarify that, in our study, a source line of code contains at least one valid character, which is not a blank space or a source code comment. In addition, we only use the Java files to calculate the SLOC, and to do so, we use the SLOCCount tool~\cite{wheeler2004:home}. 

% \alexander{What does ``its'' refer to? classes?}
% \alexander{This information is already in Table ~\ref{tab:project_details}.}
% I moved the paragraph here to address the above comment. If you prefer, I can remove it later.
 
The number of contributors and the level of activity was extracted from OpenHub, an on-line community and public directory that offers analytics, search services and tools for open source software \cite{Openhub:home}. Number of contributors is calculated by counting the different authors that committed changes to the source code repository. However, there is the possibility that one developer possesses more than one user name in the source code repository. To mitigate this risk, OpenHub provides a interface where the manager of the project on OpenHub is allowed to create user aliases to link two or more different user names belonging to the same user \cite{Openhub:Aliases}. Activity level is a metric provided by OpenHub, and it is based on a combination of contributor count and commit count, with more weight placed on contributors. These two metrics are combined so that recent activity is much more heavily weighted, \textit{i.e.,} a contribution this past month is far more important in determining activity level than a contribution a year ago \cite{Openhub:activity_level}. 

% \alexander{Do I understand correctly that a sequence of line comments would be considered as one block comment? Apparently not (see the second heuristics below). Explain the difference between the three kinds of comments. }
It is important to note that the number of comments shown for each project does not represent the number of commented lines, but rather the number of Line, Block and Javadoc comments. Line, Block and Javadoc comments are distinguished by their syntax and their location in the Java class. For example, Line comments are written with ``\textbackslash\textbackslash'' followed by the desired comment. Block comments and Javadoc comments have similar syntax. To open a Block comment ``\textbackslash\text{*}'' are used, whereas Javadoc comments are opened with ``\textbackslash\text{*}\text{*}''. Both, Block and Javadoc comments, are closed by ``\text{*}\textbackslash''. In addition, Javadoc comments are written exclusively before methods and constructors, whereas Block comments can be found scattered in the class.  

\begin{table*}[thb!]
    \begin{center}
    \caption{Details of Studied Projects}
    \label{tab:project_details}
    
            \begin{tabular}{l|rrrr||rrrr}
            \toprule
            
            \multirow{5}{*}{\textbf{\thead{Project}}} & \multicolumn{4}{c||}{\textbf{\thead{Project details}}} & \multicolumn{4}{c}{\textbf{\thead{Comments details}}} 

            \\
            \cmidrule{2-9}

            & \textbf{\thead{\# of files}} & \textbf{\thead{SLOC}} & \textbf{\thead{\# of file\\versions}} & \textbf{\thead{\# of \\contributors}}  & \textbf{\thead{\# of \\comments}}   & \textbf{\thead{\# of \\comments \\after filtering}} & \textbf{\thead{\# of \\TD \\comments}}  & \textbf{\thead{\# of \\unique TD \\comments}}\\ 
            \midrule 
            \textbf{Camel}     & 15,091 & 800,488 & 254,920 & 289  &  1,634,361 &   700,412  &  20,141 &  4,331   \\
            \textbf{Gerrit}    &  3,059 & 222,476 &  53,298 & 270  &  1,018,006 &   129,023  &   4,810 &   271    \\
            \textbf{Hadoop}    &  8,466 & 996,877 &  79,232 & 160  &  2,512,673 &  1,172,051 &  18,927 &  1,164   \\
            \textbf{Log4j}     &  1,112 & 30,287  &  12,609 & 35   &    248,276 &    61,690  &   1,893 &   135    \\
            \textbf{Tomcat}    &  3,187 & 297,828 &  46,716 & 32   &  2,336,653 &  1,081,492 &  26,725 &  1,317   \\  
           % \textbf{Ant}    & 1,475 & 115,881 & 68,115 & 74 & 1,335,705 & 342,402 & 10,729 &  854 \\
            %\textbf{Jmeter}    & 1,181 & 81,307 & 38,091 & 33 & 1,033,390 & 441,780 & 21,356 & 1,260    \\
            \bottomrule             
        \end{tabular}
    \end{center}
\end{table*}

\subsection{Checkout All Versions of Files}
\label{sub:checkout_all_versions_of_files}


% \alexander{Does this 90\% threshold distinguish between whitespace, comments, source code or is this just 90\% of the text? What happens if the source code reflecting TD has been removed but the corresponding comment is still around? What if this happens otherwise?}
% R: Whitespace or blank lines are not considered when calculating the similarity theshold. However, comments and source code are. Just remembering that this comparisson is done only in the case of file move or file rename. About the consistency between changes including comments and source code we have related work that shows that they are consistent in 90% of the cases. 


Source code comments can be added and/or removed throughout the lifetime of the project. This means that \SATD comments are being removed and introduced during the evolution of the software project. Therefore, analyzing a single version of a project is not enough to study the behavior of \SATD comments. Therefore, we extract all file versions in order to study when the \SATD is introduced and removed. First, we identify all Java source code files currently available in the latest version of the project. Then, we analyze the source code repository to track all changes done to each file. Each change made to a file produces a different version of that file, and by extracting them we can analyze each file version looking for \SATD. Git is capable of tracking renamed or moved files based on a similarity threshold~\cite{BirdMSR2009,HataIWPSE-EVOL2011}. In our study, we use this similarity threshold at 90\%. For example, if a file is renamed or moved to another folder, and is at least 90\% similar to an older version, Git will consider that the file was just moved or renamed. However, if after the change the file is less than 90\% similar with the previous version Git will consider that the original file was removed and a new one was created instead. It is important to note that whitespace or blank lines are not taken into consideration to calculate the similarity of the files. Using this feature from Git, we are able to trace and extract all versions of the Java source files that are currently in the repository. 

% \alexander{shouldn't Latin be in italics?}
The second step to checkout all versions of files is to identify the files that are not present in the repository anymore (\textit{i.e.,} deleted files). To do so, we again rely on a feature from Git that shows us all the files that have been removed from the repository. By parsing the output that Git provides us we can obtain the commit hash and the fully qualified name of the file. Using this information we extract the deleted file and repeat the process described above to obtain all the older versions of this file. To guarantee the correctness of our process while parsing the Git output for deleted files we make sure that we look only for Java source files, and that the fully qualified path of the file that we are extracting is not already in stored in our database. 
%\alexander{What happens when the project is restructured and many files are moved from one directory to another? How would this affect the results? Did this happen in the five projects studied?}\everton{Q1/2. Nothing would change. Restructuring files would not cause enough change to break the 90\% similarity threshold, and therefore, Git can track the file and keep it's history. Q3 We did not analyze if this phenomena happened in the five studied projects. What might be confusing is that JRuby, that was present in one of ours first try outs, presented this restructuring phenomena. However, that was before the tool be able to track changed files. With JRuby specifically we were not able to use the our tool because it's repository metadata was weird.}

Once this step is complete, we have at our disposal the information regarding the files and their versions stored in the database, and an actual copy of each file version in a structured directory inside our tool that will be used to extract the remainder of the data for our study.

\subsection{Parse Source Code}
\label{sub:parse_source_code}

Once we obtain all of the file versions, we parse the source code in order to extract out the comments. We use SrcML \cite{Collard2013SIE} to parse the source code files that we identified in the repository. SrcML is an open source library that parses source code files into a XML structure. The resulting XML file has tags that allow us to easily identify elements that we want to extract. This way we extract all comments from the source code files and information related to them for example, the line that each comment starts, finishes and the type of the comment (\textit{i.e.,} Javadoc, Line or Block). All the extracted information is stored in a relational database to facilitate the processing of the data. 

Once the parsing step is complete, we have at our disposal all of the comments in all versions of the files for each of the five projects. Next, we use the technique presented in by Maldonado \emph{et al.}~\cite{Maldonado2015TSE} to determine the \SATD comments. We refer readers to Maldonado \emph{et al.}'s paper for full details on how to identify \SATD comments, however, to make our paper self sufficient, we highlight the key points of their approach next.

\subsection{Using NLP to Identify Self-Admitted Technical Debt}

The state-of-the-art technique to identify \SATD uses Natural Language Processing (NLP) to classify comments based on their contents. However, before classifying comments, one needs to remove comments that are irrelevant. 

\noindent\textbf{Filtering Irrelevant Comments.} As the prior work showed, not all comments can contain \SATD \todo{add cite}. Therefore, we remove the following types of comments from our dataset before applying the NLP-based classification to determine \SATD commetns:

\begin{enumerate}
\item \textbf{License comments} are not very likely to contain self-admitted technical debt, and are commonly added before the declaration of the class. We create a heuristic that removes comments that are placed before the class declaration. Since we know the line number that the class was declared we can easily check for comments that are placed before that line and remove them. In order to decrease the chances of removing a self-admitted technical debt comment while executing this filter we calibrated this heuristic to not remove comments containing one of task annotations (\textit{i.e.,} ``TODO:'', ``FIXME:'', or ``XXX:'') ~\cite{Storey2008ICSE}. Task annotations are an extended functionality provided by most of the popular Java \textit{IDEs} including Eclipse, InteliJ and NetBeans. When one of these words are used inside a comment the IDE will automatically keep track of the comment creating a centralized list of tasks that can be conveniently accessed later on. \alexander{I miss here something like ``We consider task annotations as \SATD.'' Or ``Following ... we consider...''}

\item \textbf{Long comments} that are created using multiple \emph{Single-line} comments instead of a \emph{Block} comment can hinder the understanding of the message considering the case that the reader (\textit{i.e.,} human or machine) analyzes each one of these comments independently. To solve that problem, we create a heuristic that searches for consecutive Single-line comments and groups them as one \alexander{Block?} comment.\everton{Block could be confusing because it is a specific entity of the Java language with its own syntax. What we have here is more like an group of single line comments. But, if you think that block makes the text clearer to the reader I am all in.}

\item \textbf{Commented source code} is found in the projects due to many different reasons. One of the possibilities is that the code is not currently being used. Other is that, the code is used for debugging purposes only. Based on our analysis, commented source code does not have self-admitted technical debt.\alexander{``Our analysis'' is very vague here: what has been analyzed and how? What kind of results did we obtain?} \everton{I agree that analysis here is not ideal. This was part of one of our first studies when I joined Concordia. It was a course project, and basically, after we extracted source code comments from ten projects (same projects from TSE) we manually read through them to evaluate the results. Some patterns, became very clear. For example, all classes from one project had license comments. These license comments does not contains TD , so we removed. That is how we created our heuristics}Our heuristic removes commented source code using a simple regular expression that captures typical Java code structures.

\item \textbf{Automatically generated comments} by the IDE are filtered out as well. These comments are inserted as part of code snippets used to generate constructors, methods and try catch blocks, and have a fixed format (\textit{i.e.,} ``Auto-generated constructor stub'', ``Auto-generated method stub'', and ``Auto-generated catch block'' \alexander{is this an extensive list or are these merely examples?}).\everton{An extensive list.} Therefore, our heuristic searches for these automatically generated comments and removes them. 

\item \textbf{Javadoc comments} rarely mention self-admitted technical debt. For the Javadoc comments that do mention self-admitted technical debt, we notice that they usually\alexander{how often?} \everton{Not often at all, but not quantified yet. To quantify, query the number of comments from table processed\_comment where type = Javadoc and classification is different than WITHOUT\_CLASSIFICATION.}
 contain one of the task annotations (\textit{i.e.,} ``TODO:'', ``FIXME:'', or ``XXX:''). Therefore, our heuristic removes all comments of the type Javadoc unless they contain at least one of the task annotations  To do so, we create a simple regular expression that searches for the task annotations before removing the comment.  
 
\end{enumerate} 


The steps mentioned above significantly reduced the number of comments in our dataset and helped us focus on the most applicable and insightful comments.\alexander{how did we ensure that these were the most ``applicable and insghtful''?} \everton{This statement was intuitively made. Here we are not trying to say that the removed comments were not important, we are trying to say that from our perspective (that is classifying TD comments) removing noisy comments (comments that does not have TD) leaves us with the most applicable comments (for the task)} For example, in the Camel project, applying the above steps helped to reduce the number of comments from 1,634,361 to 700,412 meaning a reduction of 57.1\% in the number of comments to be classified. \alexander{Refer to the table above} Using the filtering heuristics we were able to eliminate between 53.3\% to 87.3\% of all comments. Table \ref{tab:project_details} provides the number of comments kept after the filtering heuristics for each project.

%\subsection{Filtering Comments}
%\label{sub:filtering_comments}

%Source code comments can be used for different purposes in a project, such as giving context, documenting, expressing thoughts, opinions and authorship, and in some cases, disabling source code from the program.\alexander{This is a nice statement but it somehow seems to call for a bibliographic reference.} \rabe{not sure about this \cite{Ying2005MSR}} \everton{This statement was me thinking of how I have used comments in the past. Nevertheless, the same statement now is published in the TSE. I do not know if this counts though.}Comments are used freely by developers and with limited formalities, if any at all. This informal environment allows developers to bring to light opinions, insights and even confessions (e.g., self-admitted technical debt). 

%Part of these comments can be identified as self-admitted technical debt \cite{Potdar2014ICSME}, \alexander{Would you consider the comments themselves as \SATD or merely as admissions, or indications that the technical debt is somewhere around the comment?} \everton{Mostly the second option. For example, Fluri et al. in ``Do code and comments co-evolve? on the relation between source code and comment changes'' states that 97\% of the comment changes are consistent (i.e., the code comment is changed with the related piece of code). However, for the remainder of the cases I would consider option 1. }but they are not the majority of cases. With that in mind, we develop and apply five filtering heuristics \alexander{explicitly number the heuristics below} to narrow down the comments eliminating the ones that are less likely to be classified as self-admitted technical debt. 




%\alexander{I miss here an evaluation of the approach. The fact that filtering reduces the number of comments is hardly surprising. Can you test these heuristics on a set of comments that you have not seen when designing the heuristics?}\rabe{the heuristics filtering could be considered as prepossessing step}\everton{The heuristics can be applied on any Java project. They can be used because they address common usage patterns of code comments in Java projects. The results, (number of comments filtered) will vary from project to project though.}

%\subsection{NLP Classification}
%\label{sub:nlp_classification}

\emad{stopped here}
\noindent\textbf{Applying the NLP Classifier.} Next, we use the classified \SATD comments as a training dataset for the Stanford NLP Classifier~\cite{Manning2014ACL}.
An NLP Classifier, in general, takes as input a number of data items along with a classification for each data item, and automatically generates \textit{features} (\textit{i.e.,} words) from each \textit{datum}, which are associated with positive or negative numeric \textit{votes} for each class. The weights of the features are learned automatically based on the manually classified training data items (supervised learning). The NLP Classifier builds a \textit{maximum entropy model}~\cite{nigam1999using}, which is equivalent to a multi-class regression model, and it is trained to maximize the conditional likelihood of the classes taking into account feature dependences when calculating the feature weights.

In our case, the training dataset is composed of source code comments and their corresponding manual classification. The manually classified comments are part of a bigger dataset of \SATD comments created during \todo{ours previous studies}~\cite{Maldonado2015MTD,Maldonado2015TSE}. Basically, \todo{during these previous works}, we created a public available dataset containing 62,566 comments extracted from \todo{ten} \emad{why not seven / or five}\everton{Because this dataset is based on the manually classified comments used in our previous study. We could had used any number of comments (or projects) that we wanted. We choose to use all available.}open source projects. These comments were classified as \SATD comments or as regular comments (\textit{i.e.,} comments without technical debt). The dataset was classified by the first author and later, to mitigate the risk of bias, another student was asked to classify a statistically significant sample of the dataset. The Cohen's kappa coefficient~\cite{cohen1960coefficient} (\textit{i.e.,} the level of agreement between both reviewer) was of +0.81. The resulting coefficient is scaled to range between -1 and +1, where negative value means poorer than chance agreement, zero indicates exactly chance agreement, and positive value indicates better than chance agreement~\cite{fleiss1973equivalence}. Although we were able to find different types of \SATD \todo{in previous studies} we focus our training dataset on \emph{design \SATD} as it is the most common and impactful\rabe{important source of technical debt}~\cite{Ernst2015FSE}. \alexander{is ``impactful'' a word?}\everton{Not sure, but I found it on Mac's dictionary with the following definition: having a major impact or effect: an eye-catching and impactful design.}

Lastly, after training the NLP classifier, we process all comments that we extracted from the projects that we are analyzing. The resulting computation of this process is a classified source code comment. The classification can either be ``WITHOUT\_CLASSIFICATION'', meaning  that the classifier did not classify the comment as \SATD or ``DESIGN'' meaning that the classifier identified the comment as \SATD.

\alexander{what is the relation between the heuristics described in the previous section and the NLP classification here? Are those competing approaches? Complementary ones?}\rabe{the heuristics described in the previous section is considered to be a preprocessing step}\everton{In this study we could opt for not using the heuristics. However, we would expend an awful amount of time classifying comments that are not likely to have TD. Is more about feasibility than anything else. (Even in the previous study it was considered necessary to make the task of manual classification possible)}

\emad{I am missing here the evaluation of the classifier (precision/ recall) }\everton{We have all the precision and recall results in the TSE paper}


% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{multirow}
\begin{table}[t]
	\centering
	\caption{Background of participants in online survey}
	\label{survey_responses}
	\begin{tabular}{@{}l|p{1.2in}|l|l|l@{}}
		\toprule
		\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Developer\\ Role\end{tabular}}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Developer\\ Task\end{tabular}}} & \multicolumn{3}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Developer Exp. \\ (in years)\end{tabular}}} \\ \cmidrule(l){3-5} 
		&  & \textbf{1-2} & \textbf{3-5} & \textbf{\textgreater5} \\ \midrule
		\begin{tabular}[c]{@{}c@{}}A Contributor\\ Developer (8)\end{tabular} & BF, BF, NF, OTH, BF, BF, OTH, NF & 1 & 0 & 7 \\
		\begin{tabular}[c]{@{}l@{}}A Core\\ Developer (6)\end{tabular} & NF, BF, OTH, NF, CR, NF & 0 & 1 & 5 \\ \bottomrule
			
			
	\end{tabular}
	BF=Bug Fixing, NF=New Feature, CR=Code Review, OTH=Other.
\end{table}




\subsection{Survey Design}
\label{Survey_Design}
To understand the type of activities that lead to the introduce and remove of self-admitted technical debt, we designed and sent an online survey. The survey includes three main sections; I)  questions regarding 
participant role and development tasks in the projects, and experiences, II) Three multiple choice, likert scale, questions about how often developers encounter, add, and address self-admitted technical debt. III) Two open ended questions about why do developers add or remove self-admitted technical debt. To identify the population of our survey, we collected the names and emails of all developers who added or removed self-admitted technical debt detected from the previous sections. In total, we found 250 unique developers from the studied seven open source projects and we successfully sent the survey to 188 developers.  

We received 14 responses (7.4\% response rate). Table~\ref{survey_responses} shows the participants role in the projects, their main development tasks and developer experiences. Of the 14 participants, 8 (57\%) of them identified themselves as core developer, and 6 (43\%) are contributors to the projects. Five of the 14 participants work on fixing bug, and five work on implementing new features. Only one participant has the task of code review. Another 3 participants (21\%) indicted having different tasks (e.g., project user). Approximately 86\% of the participants have more than five years of development experiences, and two participants have less than or equal to five yeas of development experiences.



% \subsection*{Find Technical Debt Authors}
% \label{sub:find_technical_debt_authors}

% Once we have classified all comments as \SATD or non \SATD we can search for the authors responsible for the introduction and eventual removal of the \SATD comments. As described in subsection \ref{sub:checkout_all_versions_of_files} we have stored all different versions of all source code files. Therefore, to find the author who introduced a \SATD comment we look into the oldest version of the file containing the analyzed comment, and incrementally search through all future versions of that file until we find the first time that the comment appears for the first time. Once found, we keep tracking the \SATD comment in the remaining future versions of the file checking if  the \SATD comment was removed. 

% Additionally, when searching for authors who removed \SATD comments we have to take into consideration deleted files. As we also identify the files that once was removed we can check if the last version of the file before removal contained \SATD comments. If that is the case, the author who removed the file is also the author who removed the \SATD comments. 
