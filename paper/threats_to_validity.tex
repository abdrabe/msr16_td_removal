Following common guidelines for empirical studies~\cite{yin2013case},  this section discusses the threats to validity of our study.

\subsection{Internal Validity} Internal validity concerns factors that could
have influenced our results. We rely on the NLP classification to determine \SATD. As mentioned earlier, this approach is not perfect, achieving an average precision of 0.72 and recall of 0.56. Although the precision and recall values are not very high, the NLP technique is considered the state-of-the-art in detecting \SATD. The NLP technique outperforms the comment-patterns technique, which all prior work was built on top of (i.e., \cite{Wehaibi2016SANER,Bavota2016MSR,Potdar2014ICSME}) by 230\%, on average. We train the Stanford NLP classifier on manually tagged \SATD comments provided in prior work~\cite{Maldonado2015TSE}. The manually classified comments have been verified and published in peer-reviewed venues. 

\rabe{We also analyzed mirrored repositories and not the original SVN, because getting data from the original SVN may not be always possible as this data is just not publicly available.}

To understand the type of activities that lead to the introduce and remove of self-admitted technical debt, we conducted a online survey. We sent the survey to 188 developers who responsible for adding and removing self-admitted technical debt, and we received 14 (7.4\%) responses which may be considered small. However, a 7.4\% response rate is considered to be an acceptable response rate in questionnaire-based software engineering surveys~\cite{singer2008software}.

\subsection{Construct Validity} Threats to constructed validity concern the relationship between theory and observation.
\rabe{To identify \SATD in a project, we use source code comments that describe part of the source code containing technical debt. One threat of using source code comment is the consistency of changes between the comments and the code. However, previous work shows that approximately 64\% of changes of source code is consistent with change in comments \cite{Potdar2014ICSME}.}
To identify the removed and added self-admitted technical debt, we consider commits as a single unit of change. However, a single commit may contains other source code changes. Moreover, we rely on Open-hub's data to merge developer identities, hence, our study is only as accurate as Open-hub's classification.

\subsection{External Validity} Threats to external validity concern the generalization of our findings. Our study is conducted on five large open source projects and contains more than 5,700 comment removals. That said, our findings may not generalize to other open source or commercial systems.