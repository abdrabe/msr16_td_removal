\noindent~\emph{\textit{Internal Validity:}} We rely on the NLP classification to determine \SATD. As mentioned earlier, this approach is not perfect, achieving an average precision of 0.72 and recall of 0.56. Although the precision and recall values are not very high, the NLP technique is considered the state-of-the-art in detecting \SATD. The NLP technique outperforms the comment-patterns technique, which all prior work was built on top of (i.e., \cite{Wehaibi2016SANER,Bavota2016MSR,Potdar2014ICSME}) by 230\%, on average. We train the Stanford NLP classifier on manually tagged \SATD comments provided in prior work. The manually classified comments have been verified and published in peer-reviewed venues. 

To understand the type of activities that lead to the introduce and remove of self-admitted technical debt, we conducted a online survey. We sent the survey to 188 developers who responsible for adding and removing self-admitted technical debt, and we received 14 (7.4\%) responses which may be considered small. However, a 7.4\% response rate is considered to be an acceptable response rate in questionnaire-based software engineering surveys~\cite{singer2008software}.

\noindent~\emph{\textit{Construct Validity:}}
To identify the removed and added self-admitted technical debt, we consider commits as a single unit of change. However, a single commit may contains other source code changes. Moreover, we rely on Open-hub's data to merge developer identities, hence, our study is only as accurate as Open-hub's classification.

\noindent~\emph{\textit{External Validity:}} Our study is conducted on five large open source projects and contains more than 5,700 comment removals. That said, our findings may not generalize to other open source or commercial systems.